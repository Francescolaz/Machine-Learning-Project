{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nested Cross-Validation\n",
    "\n",
    "*Nested Cross-Validation* algorithm built from scratch under the form of the `nested_cv()` Python function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nested_cv(features, labels, f_scaling, grid_alpha, k_inner, k_outer, seed, include_intercept = True):\n",
    "    \n",
    "    # Arrays for tracking outer Training, Validation errors e the best Hyperparametrs tuned in the inner loop\n",
    "    outval_errors = np.zeros((1, k_outer))\n",
    "    innval_errors = np.zeros((len(grid_alpha), k_inner))\n",
    "    bestinn_alphas = np.zeros((1, k_outer))\n",
    "    \n",
    "    # Random shuffle of the Features and Labels\n",
    "    idx_nested = np.random.RandomState(seed = seed).permutation(len(features))\n",
    "    feat_shuff, labl_shuff = features.loc[idx_nested, :], labels.loc[idx_nested, :]\n",
    "    \n",
    "    # Features and Labels divided in \"k_outer\" number of folds\n",
    "    feat_out_folds, labl_out_folds = np.array_split(feat_shuff, k_outer), np.array_split(labl_shuff, k_outer)\n",
    "    \n",
    "    # Loop on the outer Training and Validation Folds\n",
    "    for i in range(0, k_outer):\n",
    "        \n",
    "        # Features and Labels in the outer Validation fold at each iteration\n",
    "        outf_val = feat_out_folds[i]\n",
    "        outl_val = labl_out_folds[i]\n",
    "        \n",
    "        # Features and Labels in the outer Training folds at each iteration\n",
    "        outf_train = feat_shuff.drop(outf_val.index)\n",
    "        outl_train = labl_shuff.drop(outl_val.index)\n",
    "        \n",
    "        # Training Features and Labels divided in \"k_inner\" number of folds\n",
    "        feat_inn_folds, labl_inn_folds = np.array_split(outf_train, k_inner), np.array_split(outl_train, k_inner)\n",
    "        \n",
    "        #Â Loop on the values of the Hyperparameter in the grid \n",
    "        for alp in range(0, len(grid_alpha)):\n",
    "            \n",
    "            # Loop on the inner Training and Validation folds\n",
    "            for inn in range(0, k_inner):\n",
    "                \n",
    "                # Features and Labels in the inner Validation fold at each iteration\n",
    "                innf_val = feat_inn_folds[inn]\n",
    "                innl_val = labl_inn_folds[inn]\n",
    "                \n",
    "                # Features and Labels in the inner Training folds at each iteration\n",
    "                innf_train = outf_train.drop(innf_val.index)\n",
    "                innl_train = outl_train.drop(innl_val.index)\n",
    "                \n",
    "                # Transform the Features without breaking the independence between Training and Validation inner folds\n",
    "                if f_scaling is not None:\n",
    "                    transformer = features_transformation(scale_transform = f_scaling)\n",
    "                    innf_train = transformer.fit(innf_train)\n",
    "                    innf_val = transformer.test_transform(innf_val)\n",
    "                \n",
    "                # Train the predictor on the inner Training folds\n",
    "                ridge_nested = Ridge(alpha = grid_alpha[alp], intercept = include_intercept)\n",
    "                ridge_nested.fit(innf_train, innl_train)     \n",
    "                \n",
    "                # Compute the inner Validation Error according to the square loss\n",
    "                innval_pred = ridge_nested.predict(innf_val)\n",
    "                innval_errors[alp, inn] = np.mean((innval_pred - innl_val)**2)\n",
    "        \n",
    "        # Retrieve the best Hyperparameter from the inner loop according to the corresponding inner Validation Error\n",
    "        innval_mean = np.mean(innval_errors, axis = 1)\n",
    "        bestinn_alphas[0,i] = grid_alpha[np.where(innval_mean == np.min(innval_mean))]\n",
    "        \n",
    "        # Transform the Features without breaking the independence between Training and Validation outer folds\n",
    "        if f_scaling is not None:\n",
    "            transformer = features_transformation(scale_transform = f_scaling)\n",
    "            outf_train = transformer.fit(outf_train)\n",
    "            outf_val = transformer.test_transform(outf_val)\n",
    "        \n",
    "        # Train the predictor with the best Hyperparameter on the outer Training folds\n",
    "        ridge_outer = Ridge(alpha = bestinn_alphas[0,i], intercept = include_intercept)\n",
    "        ridge_outer.fit(outf_train, outl_train)\n",
    "        \n",
    "        # Compute the outer Validation Error according to the square loss        \n",
    "        outval_pred = ridge_outer.predict(outf_val)\n",
    "        outval_errors[0, i] = np.mean((outval_pred - outl_val)**2)\n",
    "            \n",
    "    # Compute the average of the Outer Validation Errors as an estimate for the risk\n",
    "    nestedcv_score = np.mean(outval_errors)\n",
    "    print(\"Nested CV risk estimate is: \")\n",
    "        \n",
    "    return(np.format_float_scientific(nestedcv_score))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Machine_Learning",
   "language": "python",
   "name": "machine_learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
