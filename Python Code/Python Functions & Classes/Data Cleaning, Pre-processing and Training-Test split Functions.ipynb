{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Functions created from scratch\n",
    "\n",
    "Used in the \"*California Housing - Github Code*\" Python Notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove missing values from the whole dataset\n",
    "\n",
    "def remove_null(dataset):\n",
    "    \n",
    "    # Loop over the features\n",
    "    for i in dataset:\n",
    "        \n",
    "        # Skip features with no missing values\n",
    "        if dataset[i].isnull().sum().sum() == 0:\n",
    "            continue\n",
    "        \n",
    "        # Drop the rows with missing values\n",
    "        else:\n",
    "            nan_rows = dataset[dataset[i].isnull()]\n",
    "            dataset = dataset.drop(nan_rows.index, axis=0)\n",
    "    \n",
    "    # Update the dataset\n",
    "    dataset = pd.DataFrame.reset_index(dataset, drop=True)\n",
    "    \n",
    "    return(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class to apply the three transformation methods to the features\n",
    "# +++ Dummies should not be transformed, they are recognized by the algorithm by min = 0 and max = 1 +++\n",
    "\n",
    "class features_transformation(): \n",
    "      \n",
    "    def __init__(self, scale_transform): \n",
    "                \n",
    "        self.scale_transform = scale_transform\n",
    "        \n",
    "          \n",
    "    # Transform the features of the training set       \n",
    "    def fit(self, features):\n",
    "        \n",
    "        # Save the columns name of the DataFrame\n",
    "        features_col = features.columns\n",
    "        \n",
    "        # Transform the DataFrame in matrix\n",
    "        features = np.matrix(features)\n",
    "        \n",
    "        # Apply standardization\n",
    "        if self.scale_transform == 'standardization':\n",
    "            mean_par = np.mean(features, axis=0)\n",
    "            std_par = np.std(features, axis=0)\n",
    "            \n",
    "            for i in range(0, std_par.shape[1]):\n",
    "\n",
    "                # Do NOT standardize the dummies\n",
    "                if np.min(features[:, i]) == 0.0 and np.max(features[:, i]) == 1.0:\n",
    "                    mean_par[0, i] = 0.0\n",
    "                    std_par[0, i] = 1.0\n",
    "            \n",
    "            new_features = np.divide((features - mean_par), std_par)\n",
    "            new_features = pd.DataFrame(new_features, columns = features_col)\n",
    "            \n",
    "            # Save standardization parameters (mean, standard deviation) of the training features\n",
    "            self.mean_par = mean_par\n",
    "            self.std_par = std_par\n",
    "            \n",
    "        # Apply Normalization\n",
    "        elif self.scale_transform == 'normalization':\n",
    "            min_par = np.min(features, axis=0)\n",
    "            max_par = np.max(features, axis=0)\n",
    "            \n",
    "            for a in range(0, max_par.shape[1]):\n",
    "                \n",
    "                # Do NOT normalize the dummies already in [0, 1]\n",
    "                if np.min(features[:, a]) == 0.0 and np.max(features[:, a]) == 1.0:\n",
    "                    max_par[0, a] = 1.0\n",
    "                    min_par[0, a] = 0.0\n",
    "            \n",
    "            new_features = np.divide((features - min_par), (max_par - min_par))\n",
    "            new_features = pd.DataFrame(new_features, columns = features_col)\n",
    "            \n",
    "            # Save normalization parameters (min, max) of the training features\n",
    "            self.min_par = min_par\n",
    "            self.max_par = max_par\n",
    "        \n",
    "        # Applying Unit-length Scaling\n",
    "        elif self.scale_transform == 'unit_length':\n",
    "            unit_norm = np.zeros((1, features.shape[1]))\n",
    "            \n",
    "            for p in range(0, features.shape[1]):\n",
    "                \n",
    "                unit_norm[0, p] = LA.norm(features[:, p], ord = 1, axis = 0)\n",
    "                \n",
    "                # Do NOT scale the dummies\n",
    "                if np.min(features[:, p]) == 0.0 and np.max(features[:, p]) == 1.0:\n",
    "                    unit_norm[0, p] = 1.0\n",
    "            \n",
    "            new_features = np.divide(features, unit_norm)\n",
    "            new_features = pd.DataFrame(new_features, columns = features_col)\n",
    "            \n",
    "            # Save unit-length scaling parameters (norm) of the training features\n",
    "            self.unit_norm = unit_norm\n",
    "            \n",
    "        # No transformation of the traning features is required\n",
    "        else:\n",
    "            new_features = features\n",
    "            \n",
    "        return(new_features)\n",
    "    \n",
    "    \n",
    "    # Transform the features of the test set with previous transformation parameters\n",
    "    def test_transform(self, test_features):\n",
    "        \n",
    "        test_features_col = test_features.columns\n",
    "        \n",
    "        # Transform the DataFrame in matrix\n",
    "        test_features = np.matrix(test_features)\n",
    "        \n",
    "        # Apply standardization\n",
    "        if self.scale_transform == 'standardization':\n",
    "            new_test_features = np.divide((test_features - self.mean_par), self.std_par)\n",
    "            new_test_features = pd.DataFrame(new_test_features, columns = test_features_col)\n",
    "            \n",
    "        # Apply normalization\n",
    "        elif self.scale_transform == 'normalization':\n",
    "            new_test_features = np.divide((test_features - self.min_par), (self.max_par - self.min_par))\n",
    "            new_test_features = pd.DataFrame(new_test_features, columns = test_features_col)\n",
    "            \n",
    "        # Apply unit-length scaling\n",
    "        elif self.scale_transform == 'unit_length':\n",
    "            new_test_features = np.divide(test_features, self.unit_norm)\n",
    "            new_test_features = pd.DataFrame(new_test_features, columns = test_features_col)\n",
    "            \n",
    "        # No transformation of the testing features is required\n",
    "        else:\n",
    "            new_test_features = test_features\n",
    "        \n",
    "        return(new_test_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training/Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the whole dataset in training/test set\n",
    "\n",
    "def train_test(features, labels, test_set_proportion, seed):\n",
    "    \n",
    "    # Set a seed for the reproducibility of the results\n",
    "    rd.seed(a=seed, version=2)\n",
    "    \n",
    "    # Retrieve the indexes of the training data points\n",
    "    train_sample = rd.sample(features.index.tolist(), int(features.shape[0]*(1-test_set_proportion)))\n",
    "    features_train = features.iloc[train_sample, :]\n",
    "    \n",
    "    # Retrieve the indexes of the testing data points\n",
    "    test_sample = np.delete(features.index.tolist(), features_train.index)\n",
    "    features_test = features.iloc[test_sample, :]\n",
    "    \n",
    "    # Retrieve the training and test labels\n",
    "    labels_train = labels.iloc[train_sample,:]\n",
    "    labels_test = labels.iloc[test_sample, :]\n",
    "\n",
    "    # Re-counting the indexes of the variables from zero\n",
    "    features_train = features_train.reset_index(drop = True) \n",
    "    labels_train = labels_train.reset_index(drop = True)\n",
    "    \n",
    "    features_test = features_test.reset_index(drop = True)\n",
    "    labels_test = labels_test.reset_index(drop = True)\n",
    "    \n",
    "    return(features_train, features_test, labels_train, labels_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Machine_Learning",
   "language": "python",
   "name": "machine_learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
